{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fab14abd-7d15-4f5c-90f7-216892e4ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "245d899c-257f-41a4-af44-363beabab229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_batch_size, test_batch_size):\n",
    "    # Fetch training data: total 60000 samples\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train = True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((32, 32)),\n",
    "                           transforms.ToTensor()\n",
    "                       ])),\n",
    "        batch_size = train_batch_size, shuffle=True)\n",
    "\n",
    "    # Fetch test data: total 10000 samples\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train = False, transform=transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor()\n",
    "        ])),\n",
    "        batch_size = test_batch_size, shuffle=True)\n",
    "\n",
    "    return (train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a68fbb88-0834-48ef-a54f-c1697d855deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_List=[505,1000,2500,5000,7000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8aaa953b-567c-4dfd-b127-e11e44209bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # flatten as one dimension\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ce825f4-a379-467c-9710-f3348a044262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,num_epochs,train_load):\n",
    "    model.train()\n",
    "    train_load=train_load\n",
    "\n",
    "    n_total_steps = len(train_load)\n",
    "    train_losses = []\n",
    "    train_epoch = []\n",
    "    train_acc = []\n",
    "    not_converged =True\n",
    "    epoch = 0\n",
    "    sensitivity=[]\n",
    "    while not_converged:\n",
    "        epoch += 1\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        lossSum =0\n",
    "        for i, (imgs, targets) in enumerate(train_load):\n",
    "\n",
    "            imgs, targets = Variable(imgs),Variable(targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            prediction = model(imgs)\n",
    "\n",
    "            imgs.requires_grad = True\n",
    "\n",
    "            loss = loss_function(prediction, targets)\n",
    "            lossSum += loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # sensitivity\n",
    "            froGrad=0\n",
    "            count =0\n",
    "\n",
    "\n",
    "            for p in model.parameters():\n",
    "                grad = 0.0\n",
    "                if p.grad is not None:\n",
    "                    grad = p.grad\n",
    "                    froGrad_norm = torch.linalg.norm(grad).numpy()\n",
    "                    froGrad += froGrad_norm\n",
    "                    count += 1\n",
    "\n",
    "            sensitivity.append(froGrad/count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += targets.size(0)\n",
    "            n_correct += (predicted == targets).sum().item()\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_acc.append(acc)\n",
    "            train_epoch.append(epoch)\n",
    "\n",
    "\n",
    "            if (i+1) % 20 == 0:\n",
    "                print (f'Train O/P: Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}',end= '\\r',flush = True)\n",
    "                if epoch == num_epochs:\n",
    "                        #print(\"Max Epoch Reached\")\n",
    "                        not_converged = False\n",
    "                elif (epoch > 5) and  (train_losses[-1] < 0.001):\n",
    "                    if abs(train_losses[-3] - train_losses[-2]) < 1.0e-05 and abs(train_losses[-2] - train_losses[-1]) < 1.0e-05:\n",
    "                        print(\"Convergeance reached for loss:\",train_losses[-1])\n",
    "                        not_converged = False\n",
    "\n",
    "        trainAvgLoss = lossSum/i+1\n",
    "        print(\"Avg Training loss:\",trainAvgLoss)\n",
    "\n",
    "    return train_epoch,train_losses,train_acc,sensitivity,trainAvgLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2dfc78e-e548-4ef8-99a5-a29e8a601a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7ac0e0a-6a2d-49f4-93d1-fd6e1862cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_load):\n",
    "    test_load = test_load\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        testLoss = 0\n",
    "        count = 0\n",
    "        for imgs,targets in test_load:\n",
    "            imgs, targets = Variable(imgs),Variable(targets)\n",
    "\n",
    "            prediction = model(imgs)\n",
    "            testLoss += loss_function(prediction,targets).item()\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += targets.size(0)\n",
    "            n_correct += (predicted == targets).sum().item()\n",
    "            count += 1\n",
    "    netTest_loss = testLoss/count\n",
    "    netTest_acc1 = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test images: {netTest_acc1}% & Test Loss: {netTest_loss} ',end= '\\r',flush = True)\n",
    "    return netTest_acc1, netTest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8df9f40c-f17b-456e-9efd-a09954635095",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsTrainEpoch1 = []\n",
    "modelsTrainLoss1 = []\n",
    "modelsTrainAcc1 = []\n",
    "modelsTestLoss1 = []\n",
    "modelsTestAcc1 = []\n",
    "modelsSensitivity1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1697c2b-6e5e-48e2-8dd3-70e6731e89c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model :61706\n"
     ]
    }
   ],
   "source": [
    "tempModel = Model ()\n",
    "\n",
    "a=[]\n",
    "for k in tempModel.parameters():\n",
    "    a.append(torch.numel(k))\n",
    "\n",
    "print(f'Total no of parameters in Model :{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3a0efd9-d446-4ae5-bf1f-17249b685a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: tensor(1.7810, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1995, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1260, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0953, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0801, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0666, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(2.1748, grad_fn=<AddBackward0>) Loss: 0.05461053662002087 \n",
      "Avg Training loss: tensor(1.3155, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1955, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1444, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1159, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0993, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(3.0410, grad_fn=<AddBackward0>) Loss: 0.07900961712002755 \n",
      "Avg Training loss: tensor(1.6778, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.3988, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.2987, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.2371, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1959, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(3.4394, grad_fn=<AddBackward0>)Loss: 0.1545044556260109 \n",
      "Avg Training loss: tensor(2.8174, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.8525, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.5523, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.4472, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.3715, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.3248, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.2877, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.2534, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.2266, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.2055, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1848, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1699, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1569, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1455, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1376, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1300, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1224, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1174, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1101, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1044, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0999, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0964, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0927, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0884, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0867, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0841, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0818, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0782, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0770, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0753, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0718, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0696, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0684, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0658, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0659, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0643, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0615, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0596, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0592, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0576, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0558, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0565, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0542, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0533, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0512, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0519, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0510, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0518, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0477, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0455, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0457, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0457, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0462, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0441, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0421, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0401, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0397, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0391, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0406, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0396, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0401, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0388, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0376, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0366, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0358, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0340, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0339, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0326, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0325, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0317, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0310, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0301, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0298, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0303, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0297, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0289, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0294, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0290, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0297, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0279, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0296, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0287, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0261, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0256, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0264, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0269, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0255, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0251, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0248, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0235, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0247, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0228, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0224, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0214, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0219, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0235, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0232, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0216, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0206, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0194, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0197, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0210, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0227, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0208, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0201, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0187, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0188, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0175, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0169, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0171, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0163, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0164, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0167, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0161, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0170, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0157, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0159, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0170, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0161, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0144, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0140, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0134, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0131, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0134, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0136, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0126, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0126, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0125, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0134, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0134, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0123, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0118, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0121, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0121, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0109, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0117, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0108, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0110, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0110, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0111, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0106, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0117, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0109, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0109, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0099, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0088, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0095, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0083, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0083, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0083, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0077, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0078, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0098, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0135, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0110, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0089, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0087, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0090, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0094, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0111, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0098, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0100, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0081, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0069, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0072, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0073, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0075, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0068, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0062, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0063, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0061, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0060, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0064, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0066, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0057, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0059, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0056, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0051, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0051, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0055, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0055, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0049, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0047, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0051, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0053, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0064, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0059, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0050, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0049, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0044, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0047, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0044, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0043, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0038, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0035, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0035, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0035, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0036, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0032, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0032, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0034, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0034, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0035, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0037, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0041, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0036, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0030, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0035, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0043, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0045, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0056, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0046, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0048, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0040, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0029, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0032, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0025, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0028, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0026, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0027, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0025, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0022, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0021, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0021, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0021, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0020, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0020, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0022, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0027, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0039, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0093, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0214, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0440, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0298, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0177, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0095, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0064, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0049, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0038, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0034, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0032, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0029, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0028, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0030, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0026, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0025, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0025, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0023, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0020, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0019, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0018, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0017, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0017, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0017, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0015, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0015, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0008, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0011, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0008, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0008, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0007, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0008, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0008, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0008, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0008, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0008, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0009, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0021, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(3.1168, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.3874, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1826, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1288, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.1001, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0814, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0704, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0621, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0562, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0506, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0466, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0434, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0408, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0386, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0371, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0351, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0333, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0323, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0313, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0299, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0291, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0280, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0271, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0261, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0253, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0248, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0241, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0233, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0225, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0223, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0212, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0208, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0203, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0203, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0198, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0189, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0184, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0180, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0175, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0171, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0166, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0161, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0157, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0154, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0150, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0146, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0145, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0141, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0139, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0137, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0135, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0132, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0128, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0129, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0122, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0122, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0118, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0115, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0113, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0111, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0108, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0108, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0107, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0100, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0099, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0096, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0097, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0094, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0093, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0090, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0089, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0088, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0085, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0083, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0081, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0080, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0078, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0078, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0076, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0078, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0074, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0071, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0071, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0070, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0068, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0067, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0066, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0067, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0063, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0063, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0061, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0059, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0059, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0058, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0057, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0059, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0056, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0054, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0053, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0052, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0052, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0052, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0052, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0051, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0048, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0049, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0048, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0048, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0048, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0046, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0044, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0044, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0043, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0044, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0042, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0043, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0042, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0041, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0039, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0039, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0038, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0037, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0037, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0036, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0037, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0036, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0035, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0035, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0035, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0034, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0033, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0033, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0033, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0033, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0033, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0034, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0033, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0030, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0030, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0030, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0030, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0029, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0028, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0028, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0028, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0027, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0027, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0027, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0027, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0025, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0025, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0025, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0025, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0023, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0023, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0024, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0022, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0023, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0022, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0022, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0021, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0021, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0021, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0021, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0021, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0020, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0020, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0020, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0020, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0020, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0019, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0019, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0019, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0018, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0018, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0018, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0018, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0018, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0018, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0018, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0018, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0017, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0017, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0017, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0017, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0017, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0016, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0015, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0015, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0015, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0015, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0015, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0015, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0015, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0014, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0013, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n",
      "Avg Training loss: tensor(1.0012, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/local_scratch/slurm.712601/ipykernel_380380/394869450.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msensitivity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtvgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtestAcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_scratch/slurm.712601/ipykernel_380380/419240521.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_load)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlossSum\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    460\u001b[0m             )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected type {type(img)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ece8550/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range (len(Batch_List)):\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    j=copy.deepcopy(i)\n",
    "    j = Model()\n",
    "    #loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(j.parameters(), lr=1e-3, weight_decay= 1e-4)\n",
    "    train_batch_size = int(Batch_List[i])\n",
    "    test_batch_size = int(Batch_List[i])\n",
    "    train_load,test_load= load_data(train_batch_size,test_batch_size)\n",
    "    max_epochs = 6\n",
    "    \n",
    "    \n",
    "    train_epoch,train_losses,train_acc,sensitivity,tvgLoss = train(j,max_epochs,train_load)\n",
    "    testAcc, testLoss = test(j,test_load)\n",
    "    \n",
    "    modelsTrainEpoch1.append(train_epoch)\n",
    "    modelsTrainLoss1.append(train_losses)\n",
    "    modelsTrainAcc1.append(train_acc)\n",
    "    modelsTestAcc1.append(testAcc)\n",
    "    modelsTestLoss1.append(testLoss)\n",
    "    modelsSensitivity1.append(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf46d60-c93c-490b-9c5e-68f6219d835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7314e-3268-4812-ab1e-bc278c2e25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanScore(dataArr):\n",
    "    meanModelData = []\n",
    "    for i in range (len(dataArr)):\n",
    "        meanScore = np.mean(dataArr[i])\n",
    "        meanModelData.append(meanScore)\n",
    "    return meanModelData\n",
    "\n",
    "def minScore(dataArr):\n",
    "    minModelScore = []\n",
    "    for i in range (len(dataArr)):\n",
    "        minScore = np.mean(dataArr[i])\n",
    "        minModelScore.append(minScore)\n",
    "    return minModelScore\n",
    "\n",
    "def maxScore(dataArr):\n",
    "    maxModelScore = []\n",
    "    for i in range (len(dataArr)):\n",
    "        maxScore = np.max(dataArr[i])\n",
    "        maxModelScore.append(maxScore)\n",
    "    return maxModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6231f0-54cc-4d00-8c02-679bacc425bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(batc,minScore(modelsTrainLoss1),color=\"Blue\", marker=\"o\",linestyle='dashed')\n",
    "ax.plot(batchArr,modelsTestLoss1,color=\"Blue\", marker=\"v\")\n",
    "ax.legend(['Train Loss','Test Loss'],loc=\"upper right\")\n",
    "ax.set_xlabel(\"Batch\",color=\"Green\")\n",
    "ax.set_ylabel(\"CrossEntropy Loss\",color = \"blue\")\n",
    "ax.set_title(\"Loss,Sensitivity Vs Batch size\",color=\"red\")\n",
    "\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(batchArr,minScore(modelsSensitivityArr),color=\"hotpink\")\n",
    "ax2.set_xlabel(\"Batch\",color=\"Green\")\n",
    "ax2.set_ylabel(\"Sensitivity\",color = \"hotpink\")\n",
    "ax2.legend(['Sensitivity'],loc=\"center left\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
